{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustring Sujet 2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/said-chafouai/E-Colocation-WebApplication/blob/main/Clustring_Sujet_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E_2y3jLQpww",
        "outputId": "9772acce-0c6b-4166-da4b-63e356b65501"
      },
      "source": [
        "# Import KModes\n",
        "from kmodes.kmodes import KModes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# package for detecting a file encoding\n",
        "import chardet\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "# for converting categorical values to numerical ones\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# for standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# to compute NearestNeighbors for DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# for DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# to mount Google drive in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Acces path : '/content/MyDrive/Monext/'\n",
        "gDrivePath='/content/drive/MyDrive/Monext'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY1wCVo2RLmo"
      },
      "source": [
        "### Functions\n",
        "\n",
        "########################################################################################################################\n",
        "def read_csv_file_into_df(file_path):\n",
        "    \"\"\"\n",
        "    read content of a csv file to a dataframe\n",
        "    :param file_path: csv_file to read from\n",
        "    :return: dataframe\n",
        "    \"\"\"\n",
        "    # with open(file_path, 'rb') as file:\n",
        "    #     result = chardet.detect(file.read(10000))\n",
        "    return pd.read_csv(file_path, delimiter=';', encoding='latin1')\n",
        "\n",
        "########################################################################################################################\n",
        "def write_df_into_csv_file(file_path, dataframe):\n",
        "    with open(file_path, mode='w') as file:\n",
        "        dataframe.to_csv(file, index=False, sep=';', line_terminator='\\n', encoding='latin1' )\n",
        "\n",
        "########################################################################################################################\n",
        "def Kmeans_estimator_elbow_method(X: pd.DataFrame, cluster_range: []):\n",
        "  \"\"\"\n",
        "  Performe Kmeans on the X given dataset and trace the elbow graphe\n",
        "  \"\"\"\n",
        "  inertia_list = []\n",
        "  for i in cluster_range:\n",
        "    kmeans_estimator = KMeans(n_clusters=i, random_state=10)\n",
        "    kmeans_estimator.fit(X)\n",
        "    inertia = kmeans_estimator.inertia_\n",
        "    inertia_list.append(inertia)\n",
        "    print(f'cluster {i}, Inertia {inertia}\\n')\n",
        "  \n",
        "  # Plot the squared distance for each K\n",
        "  plt.plot(cluster_range, inertia_list, 'bx-')\n",
        "  plt.show() # -> the best K is when the graphe stated to be linear\n",
        "\n",
        "########################################################################################################################\n",
        "def preprocessing_convert_df_to_numerical_values(X: pd.DataFrame):\n",
        "  \"\"\"\n",
        "  Convert each column of the dataframe X to numrical ones\n",
        "  return : X the modified Dataframe\n",
        "  \"\"\"\n",
        "  X_numpy = X.to_numpy()\n",
        "\n",
        "  for i in range(X_numpy.shape[1]):\n",
        "    label_encoder = LabelEncoder()\n",
        "    unique_values = label_encoder.fit_transform(X_numpy[:,i:i+1].ravel())\n",
        "    mapping_unique_values = {\n",
        "        index: label for index, label in enumerate(label_encoder.classes_)\n",
        "    }\n",
        "    X_numpy[:,i:i+1] = unique_values.reshape(unique_values.size,1)\n",
        "\n",
        "  X = pd.DataFrame(X_numpy, columns=X.columns)\n",
        "  X.fillna(-1) # attention n'est pas encore testée\n",
        "  return X\n",
        "\n",
        "########################################################################################################################\n",
        "# def standardaize_dataset(X: pd.DataFrame, estimator_type, ):\n",
        "#   \"\"\"\n",
        "#   standardize the given dataset\n",
        "#   X: the dataframe to standardize\n",
        "#   estimator_type: string 'supervised ou unsupervised' to know if i should standardize \n",
        "#   the last column or not \n",
        "#   \"\"\"\n",
        "#   X_numpy\n",
        "#   if estimator_type == 'supervised':\n",
        "\n",
        "#   X_standarded =  StandardScaler().fit_transform(X)\n",
        "#   return X_standarded\n",
        "########################################################################################################################\n",
        "def do_PCA(X, new_dimension):\n",
        "  \"\"\"\n",
        "  Do de PCA on the given dataset, based on new_dimension \n",
        "  return: reduced dataset\n",
        "  \"\"\"\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(2)\n",
        "  X = pca.fit_transform(X)\n",
        "  return X\n",
        "########################################################################################################################\n",
        "def filter_df(df, columns:[]):\n",
        "  \"\"\"\n",
        "  filter a dataframe\n",
        "  df: dataframe to filter\n",
        "  columns : columns to extract\n",
        "  return: dataframe\n",
        "  \"\"\"\n",
        "  X = df.filter(items=columns)\n",
        "  return X\n",
        "\n",
        "########################################################################################################################\n",
        "def do_kmeans(X: pd.DataFrame, n_clusters):\n",
        "  \"\"\"\n",
        "  do kmeans fo the given clusters\n",
        "  return: cluster_labels\n",
        "  \"\"\"\n",
        "  kmeans_estimator = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "  cluster_labels = kmeans_estimator.fit_predict(X)\n",
        "  return cluster_labels \n",
        "\n",
        "def plot_clusters_2D(df_trans, cluster_labels, figsize:tuple):\n",
        "  unique_clusters = np.unique(cluster_labels)\n",
        "  plt.figure(figsize=figsize)\n",
        "  for cluster in unique_clusters:\n",
        "    plt.scatter(df_trans[cluster_labels == cluster, 0], df_trans[cluster_labels == cluster, 1], label=cluster)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "########################################################################################################################\n",
        "def plot_clusters_threshold(trans_file_path, columns, column_weight_file_path, threshold):\n",
        "  \"\"\"\n",
        "  this function extract classes that have a number of samples greather than threshold\n",
        "  the dataset of the given csv file must \n",
        "  \"\"\"\n",
        "  df_trans = read_csv_file_into_df(file_path)\n",
        "  df_trans = read_csv_file_into_df(file_path)\n",
        "  df_trans = filter_df(df_trans, columns)\n",
        "\n",
        "########################################################################################################################\n",
        "def do_clustring_presonnalized_algo(df_trans, columns, threshold, similarity):\n",
        "  \"\"\"\n",
        "  this function do a presonnalized clustring\n",
        "  file_path: \n",
        "  columns : list of columns ordered by importance\n",
        "  threshold: show clusters with threshold elements\n",
        "  similarity: the number of columns used to calculate similarity between clusters (must be between 1 and nb_columns)\n",
        "  \"\"\"\n",
        "  df_trans['UniqueNumber'] = np.arange(df_trans.shape[0])\n",
        "  df_trans['Counter'] = 1\n",
        "  columns.append('UniqueNumber')\n",
        "  df_groupedBy = df_trans.groupby(by=columns, as_index=False)['Counter'].count()\n",
        "  df_groupedBy['Cluster'] = 0\n",
        "  \n",
        "  nb_cluster = 1\n",
        "  old_row = df_groupedBy.iloc[0, :]\n",
        "\n",
        "  if similarity > 1:\n",
        "    for index, row in df_groupedBy.iterrows():\n",
        "        if old_row[similarity - 2] != row[similarity - 2] or old_row[similarity - 1] != row[similarity - 1]:\n",
        "          old_row = row\n",
        "          nb_cluster += 1\n",
        "        df_groupedBy.at[index, 'Cluster'] = nb_cluster\n",
        "\n",
        "  elif similarity == 1: \n",
        "    for index, row in df_groupedBy.iterrows():\n",
        "        if old_row[similarity - 1] != row[similarity - 1]:\n",
        "          old_row = row\n",
        "          nb_cluster += 1\n",
        "        df_groupedBy.at[index, 'Cluster'] = nb_cluster\n",
        "\n",
        "  elif similarity < 0 or similarity > df_trans.shape[1] - 2 :  \n",
        "    print('Wrong similarity')\n",
        "\n",
        "  df_trans.drop(columns=['UniqueNumber','Counter'], inplace=True)\n",
        "  columns.remove('UniqueNumber')\n",
        "  df_groupedBy.drop(columns=['UniqueNumber'], inplace=True)\n",
        "  return df_groupedBy\n",
        "\n",
        "########################################################################################################################\n",
        "def do_dbscan(df):\n",
        "  min_pts = 2*df.shape[1] - 2 # minus two for added columns UniqueNumbers and Counter\n",
        "  df_numeric = preprocessing_convert_df_to_numerical_values(df) # categorial to numeric\n",
        "  X_standarded =  StandardScaler().fit_transform(df_numeric) # Standardization\n",
        "\n",
        "  # finding epsilon\n",
        "  min_pts = 2*df.shape[1] - 2\n",
        "  neighbors_algo = NearestNeighbors(n_neighbors=min_pts)\n",
        "  neighbors = neighbors_algo.fit(X_standarded)\n",
        "  distances, index = neighbors.kneighbors(X_standarded)\n",
        "  mean_distance = np.mean(distances)\n",
        "  epsilon = mean_distance\n",
        "\n",
        "  # process dbscan\n",
        "  dbscan_estimator = DBSCAN(eps=epsilon, min_samples=min_pts).fit(X_standarded)\n",
        "  return dbscan_estimator.labels_\n",
        "\n",
        "########################################################################################################################\n",
        "def global_dbscan(df_trans, groupBy_columns):\n",
        "  \"\"\"\n",
        "  This function is used with DBSCAN\n",
        "  file_path: \n",
        "  columns : list of columns ordered by importance\n",
        "  threshold: show clusters with threshold elements\n",
        "  similarity: the number of columns used to calculate similarity between clusters (must be between 1 and nb_columns)\n",
        "  \"\"\"\n",
        "  nb_groupBy_columns = len(groupBy_columns) \n",
        "  df_trans['UniqueNumber'] = np.arange(df_trans.shape[0])\n",
        "  df_trans['Counter'] = 1\n",
        "  df_trans_columns = list(df_trans.columns)\n",
        "\n",
        "  if nb_groupBy_columns > 0 : \n",
        "    df_trans_columns.remove(groupBy_columns)\n",
        "  \n",
        "  groupBy_columns.extend(df_trans_columns)\n",
        "\n",
        "  print('Group by columns: ', groupBy_columns) \n",
        "\n",
        "  clusters = []\n",
        "\n",
        "\n",
        "  df_groupedBy = df_trans.groupby(by=groupBy_columns, as_index=False)['Counter'].count()\n",
        "\n",
        "  old_row = df_groupedBy.iloc[0, :]\n",
        "  start_index = 0\n",
        "\n",
        "  if nb_groupBy_columns > 0:\n",
        "    for index, row in df_groupedBy.iterrows():\n",
        "      if old_row[nb_groupBy_columns - 1] != row[nb_groupBy_columns - 1]:\n",
        "        old_row = row\n",
        "        end_index = index\n",
        "        # process DBSCAN on dataset starting from start_index to end_indexy for row and for columns from nb_groupBy_columns to the end\n",
        "        labels = do_dbscan(df_groupedBy.iloc[start_index:end_index, nb_groupBy_columns:-1])\n",
        "        clusters.extend(labels)\n",
        "  else: \n",
        "    clusters.append(do_dbscan(df_trans))\n",
        "  df_groupedBy['Cluster'] = clusters\n",
        "########################################################################################################################\n",
        "  \n",
        "def mapping_dataset_to_meaning_values(df_trans):\n",
        "  mapping_columns = {\n",
        "    'ACTION': {\n",
        "      'AUTHOR+CAP':  1,\n",
        "      'AUTHOR':  2,\n",
        "      'RESET':  3,\n",
        "      'REFUND':  4\n",
        "    },\n",
        "    'PAYMENT_CARD_CODE':{\n",
        "      'CB': 1,\n",
        "      'PAYPAL': 2,\n",
        "      'PAYSAFECARD2':3,\n",
        "      'SKRILL(MONEYBOOKERS)': 4\n",
        "    },\n",
        "    'SHORT_MESSAGE':{\n",
        "      'REFUSED': 4,\n",
        "      'ACCEPTED': 1,\n",
        "      'ERROR': 4,\n",
        "      'CANCELLED': 3,\n",
        "      'ONHOLD_PARTNER': 2\n",
        "    },\n",
        "    'FRAUD_LIST':{\n",
        "      'StandardList': 1,\n",
        "      'NewCustomerList': 1,\n",
        "      'BlackList': 2\n",
        "    },\n",
        "    'FRAUD_RULE':{\n",
        "      'Cartes Business': 1,\n",
        "      'Cartes Corporate': 1,\n",
        "      'Montant 3DS-v2-FLS': 2,\n",
        "      'Cumul7j 3DS-v2-NCH': 3,\n",
        "      'OneCardNCustomers': 4,\n",
        "      'ListCountryIssuer': 5,\n",
        "      'Unicite du compte Paypal par compte joueur': 6,\n",
        "      'Paysafecard - Cumul client > 1000 EUR par jour calendaire': 7,\n",
        "      'Unicite du compte Skrill par compte joueur': 8,\n",
        "      'Paysafecard - Cumul client > 10000 EUR par mois': 9\n",
        "    },\n",
        "    'FRAUD_EXPLANATION': {\n",
        "      'AFF': 1,\n",
        "      '3DS': 1,\n",
        "      'CUC': 1,\n",
        "      'UNI': 1,\n",
        "      'CTY': 1,\n",
        "      'PSF': 1\n",
        "    },\n",
        "    'FRAUD_ACTION': {\n",
        "      'BLOCK': 5,\n",
        "      'FRICTIONLESS': 3,\n",
        "      'NONE': 1,\n",
        "      'NO_PREFERENCE': 4,\n",
        "      'noAction': 2\n",
        "    },\n",
        "    'ENROLLED_3D_SECURE':{\n",
        "      'Y': 1,\n",
        "      'N': 3\n",
        "    },\n",
        "    'AUTHENTICATED_3D_SECURE': {\n",
        "      'Y': 1,\n",
        "      'R': 2,\n",
        "      'N': 6,\n",
        "      'A': 3,\n",
        "      'U': 5\n",
        "    },\n",
        "    'SECURITY_LEVEL': {\n",
        "      'CVV': 1,\n",
        "      '3DS': 2,\n",
        "      'Aucun': 3\n",
        "    },\n",
        "    'CHARGE_BACK': {\n",
        "        'N': 1\n",
        "    },\n",
        "    'TRANSMITTER_COUNTRY':{\n",
        "        'FRA': 1\n",
        "    },\n",
        "    'BUYER_IP_COUNTRY_CODE':{\n",
        "        'FR': 1\n",
        "    }\n",
        "  }\n",
        "  df_trans['IS_DUPLICATED'].replace(to_replace='0.0', value=1, inplace=True)\n",
        "  df_trans.replace(to_replace='vide', value=0, inplace=True)\n",
        "  df_trans.replace(to_replace='-', value=0, inplace=True) # Attention à revoir\n",
        "  df_trans.replace(mapping_columns, inplace=True)\n",
        "  df_trans['EXTERNAL_RETURN_CODE'].replace(to_replace=0, value=1, inplace=True)\n",
        "  df_trans['EXTERNAL_RETURN_CODE'].where(df_trans['EXTERNAL_RETURN_CODE']==1, other=2, inplace=True)\n",
        "  df_trans['TRANSMITTER_COUNTRY'].where(df_trans['TRANSMITTER_COUNTRY']==1, other=2, inplace=True)\n",
        "  df_trans['BUYER_IP_COUNTRY_CODE'].where(df_trans['BUYER_IP_COUNTRY_CODE']==1, other=2, inplace=True)\n",
        "  write_df_into_csv_file(gDrivePath+'/fdj_trans_mapped.csv', df_trans)\n",
        "  return df_trans\n",
        "\n",
        "########################################################################################################################\n",
        "def do_kmodes_and_plot_cost(file_path, clustring_columns, groupby_columns, k):\n",
        "  \"\"\" \n",
        "  Parameters:\n",
        "    file_path : path of the csv file that contain transactions\n",
        "    clustring_columns : all imporatante columns for the tester to do clustring on\n",
        "    groupby_columns : abvious clusters column\n",
        "    K : array k[0] min clusters, k[1] max clusters\n",
        "  Return :\n",
        "    plot cost/cluster \n",
        "  \"\"\"\n",
        "  # load the csv file\n",
        "  df_trans = read_csv_file_into_df(file_path)\n",
        "  # print('### dataset shape', df_trans.shape)\n",
        "\n",
        "\n",
        "  # filter the dataset\n",
        "  df_trans = df_trans.filter(items = clustring_columns)\n",
        "  df_trans.fillna('empty', inplace=True)\n",
        "\n",
        "  df_trans = df_trans.groupby(by=clustring_columns).size().reset_index(name='Count')\n",
        "  # drop columns\n",
        "  count = df_trans['Count']\n",
        "  df_trans.drop(columns=['Count'], inplace=True)\n",
        "  # change type to string\n",
        "  df_trans = df_trans.astype('str')\n",
        "\n",
        "\n",
        "  # chose the optimal K using the elbow method\n",
        "  # run the algo form k[0](min) to k[1](max)\n",
        "  cost = []\n",
        "  rangeK = range(k[0],k[1])\n",
        "  for ith_k in rangeK:\n",
        "    try:\n",
        "      kpro_estimator = KModes(n_clusters=ith_k, n_jobs=-1, random_state=0)\n",
        "      kpro_estimator.fit_predict(df_trans)\n",
        "      cost.append(kpro_estimator.cost_)\n",
        "      print('cluster : ', ith_k)\n",
        "    except:\n",
        "      print(ith_k, ' error ')\n",
        "      cost.append(0)\n",
        "\n",
        "  # Plot the squared distance for each K\n",
        "  # plt.figure(figsize=(0.5*len(rangeK), 10))\n",
        "  plt.plot(rangeK, cost, 'bx-')\n",
        "  # plt.xticks(rangeK)\n",
        "  plt.show() # -> the best K is when the graphe started to be linear\n",
        "  return df_trans.copy(), count\n",
        "\n",
        "########################################################################################################################\n",
        "def kmodes_with_chosen_k(df_trans, count, groupby_columns, chosen_k):\n",
        "  \"\"\" \n",
        "  Parameters:\n",
        "    df_trans : grouped by dataframe, exactly the one returned by do_kmodes_and_plot_cost\n",
        "    clustring_columns : all imporatante columns for the tester to do clustring on\n",
        "    chosen_k : the optimal k chosed by the testeur\n",
        "  Return :\n",
        "    write the clustred dataframe\n",
        "  \"\"\"\n",
        "  kpro_estimator = KModes(n_clusters=chosen_k, n_jobs=-1, random_state=0)\n",
        "  kpro_estimator.fit_predict(df_trans)\n",
        "  df_trans['Cluster'] = kpro_estimator.labels_\n",
        "\n",
        "  maxCluster = chosen_k\n",
        "  clm_len = len(groupby_columns)\n",
        "  old_row = df_trans.iloc[0, :clm_len]\n",
        "\n",
        "  # comment here\n",
        "  B = [False for i in range(chosen_k)] # visited or not by previous groups\n",
        "  C = [False for i in range(chosen_k)] # visited or not by the same group\n",
        "  A = [i for i in range(chosen_k)] # clusters\n",
        "  if clm_len > 0:\n",
        "    for index, row in df_trans.iterrows():\n",
        "      # Group changes\n",
        "      if df_trans.iloc[index, :clm_len].eq(old_row).sum() < clm_len:\n",
        "        old_row = df_trans.iloc[index, :clm_len]\n",
        "        for i in range(len(C)):\n",
        "          C[i] = False\n",
        "      \n",
        "      cluster = df_trans.at[index, 'Cluster'] \n",
        "      if B[cluster] == True and C[cluster] == False:\n",
        "        A[cluster] = maxCluster\n",
        "        maxCluster += 1\n",
        "\n",
        "      B[cluster] = True\n",
        "      C[cluster] = True\n",
        "      df_trans.at[index, 'Cluster'] = A[cluster]\n",
        "  \n",
        "  df_trans['Count'] = count\n",
        "  write_df_into_csv_file(gDrivePath+'/trans_clustring_final.csv', df_trans)\n",
        "\n",
        "########################################################################################################################\n",
        "def mode_func(x):\n",
        "  return lambda x: x.value_counts().index[0]\n",
        "########################################################################################################################\n",
        "def get_centroids(file_path):\n",
        "  df_trans_clustred = read_csv_file_into_df(file_path)\n",
        "  objet = {}\n",
        "  for column in list(df_trans_clustred.columns):\n",
        "    if type(column) == str:\n",
        "      objet[column] = mode_func(column)\n",
        "  objet['Count'] = 'sum'\n",
        "\n",
        "  df_trans_clustred = df_trans_clustred.groupby(by=['Cluster'], as_index=False).agg(objet)\n",
        "  write_df_into_csv_file(gDrivePath+'/df_trans_cluster_centroids.csv', df_trans_clustred)\n",
        "\n",
        "########################################################################################################################\n",
        "def compute_coverage(file_path_centroids, file_path_tests):\n",
        "  df_tests = read_csv_file_into_df(file_path_tests)\n",
        "  df_centroids = read_csv_file_into_df(file_path_centroids)\n",
        "\n",
        "  columns_intersection = list(set(df_tests.columns) & set(df_centroids.columns))\n",
        "  df_tests = df_tests.filter(items=columns_intersection)\n",
        "  df_tests.fillna('empty', inplace=True)\n",
        "  df_centroids = df_centroids.filter(items=columns_intersection)\n",
        "  df_coverage = df_tests.copy()\n",
        "\n",
        "  for iIndex, iRow in df_tests.iterrows():\n",
        "    for jIndex, jRow in df_centroids.iterrows():\n",
        "      df_coverage.at[iIndex, str(jIndex)] = int(iRow.eq(jRow).sum())\n",
        "\n",
        "  write_df_into_csv_file(gDrivePath+'/trans_coverage.csv', df_coverage)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmvumXIYvp62"
      },
      "source": [
        "### KModes implementation\n",
        "\n",
        "# INPUT \n",
        "file_path = gDrivePath+'/fdj_transactions.csv'          # transaction file name\n",
        "# all columns you are interested on\n",
        "clustring_columns = ['ACTION', 'PAYMENT_CARD_CODE','AUTHENTICATED_3D_SECURE','FRAUD_ACTION','ENROLLED_3D_SECURE',\n",
        "                      'SECURITY_LEVEL','TRANSMITTER_COUNTRY',\n",
        "                      'BUYER_IP_COUNTRY_CODE','SHORT_MESSAGE',\n",
        "                      'FRAUD_LIST','FRAUD_EXPLANATION','CHARGE_BACK']  # - Columns that we gonna use for the clustring\n",
        "groupby_columns = ['ACTION', 'PAYMENT_CARD_CODE']    # - the columns to do group by with\n",
        "k = [1, 30]          # K : min and max clusters to preform \n",
        "\n",
        "df_trans, count = do_kmodes_and_plot_cost(file_path, clustring_columns, groupby_columns, k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnmugh1dqN_2"
      },
      "source": [
        "# Run Kmodes for choosing K\n",
        "kmodes_with_chosen_k(df_trans, count,  groupby_columns, 16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMUoRGnMUM75"
      },
      "source": [
        "# get centroids of clustred data\n",
        "file_path = gDrivePath+'/trans_clustring_final.csv'\n",
        "get_centroids(file_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXgWXiynifVO"
      },
      "source": [
        "# Compute the coverage\n",
        "file_path_tests = gDrivePath+'/fdj_test.csv'\n",
        "file_path_centroids = gDrivePath+'/df_trans_cluster_centroids.csv'\n",
        "compute_coverage(file_path_centroids, file_path_tests)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXubMh0hDueG"
      },
      "source": [
        "pip install kmodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jps000O6G2a"
      },
      "source": [
        "# Don't delete that\n",
        "groupBy_columns = []\n",
        "nb_groupBy_columns = len(groupBy_columns) \n",
        "# df_trans['UniqueNumber'] = np.arange(df_trans.shape[0])\n",
        "# df_trans['Counter'] = 1\n",
        "df_trans_columns = list(df_trans.columns)\n",
        "\n",
        "if nb_groupBy_columns > 0 : \n",
        "  df_trans_columns.remove(groupBy_columns)\n",
        "\n",
        "groupBy_columns.extend(df_trans_columns)\n",
        "\n",
        "print('Group by columns: ', groupBy_columns) \n",
        "\n",
        "clusters = []\n",
        "\n",
        "# df_groupedBy = df_trans.groupby(by=groupBy_columns, as_index=False)['Counter'].count()\n",
        "# df_groupedBy.drop(columns=['Counter', 'UniqueNumber'], inplace=True)\n",
        "\n",
        "# df_numeric = preprocessing_convert_df_to_numerical_values(df_groupedBy) # categorial to numeric\n",
        "df_numeric = mapping_dataset_to_meaning_values(df_trans)\n",
        "X_standarded =  StandardScaler().fit_transform(df_numeric) # Standardization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWPJgDmcaPJP"
      },
      "source": [
        "# finding epsilon\n",
        "min_pts = 2*df_numeric.shape[1]\n",
        "neighbors_algo = NearestNeighbors(n_neighbors=min_pts).fit(X_standarded)\n",
        "distances, index = neighbors_algo.kneighbors(X_standarded)\n",
        "mean_distance = np.mean(distances)\n",
        "epsilon = mean_distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmxsf6iFauSZ"
      },
      "source": [
        "# process dbscan\n",
        "dbscan_estimator = DBSCAN(eps=epsilon, min_samples=min_pts).fit(X_standarded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bo5I5WngyJA"
      },
      "source": [
        "# df_groupedBy['Cluster'] = dbscan_estimator.labels_\n",
        "df_trans['Cluster'] = dbscan_estimator.labels_ \n",
        "# np.min(distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOAfjRE3hP2s"
      },
      "source": [
        "write_df_into_csv_file(gDrivePath+'/df_trans_dbscan_clusterd.csv', df_trans)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lzg-S8JIoWL"
      },
      "source": [
        "df_reduced = do_PCA(df_numeric, 2)\n",
        "plot_clusters_2D(df_reduced, df_groupedBy['Cluster'], (10, 20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QFl8ZHaWIyb"
      },
      "source": [
        "### Clustrin/ Grouping by algorithm\n",
        "file_path = gDrivePath+'/fdj_trans_filterd_columns.csv'\n",
        "df_trans = read_csv_file_into_df(file_path)\n",
        "columns = ['ACTION','PAYMENT_CARD_CODE','AUTHENTICATED_3D_SECURE',\n",
        "            'FRAUD_ACTION','ENROLLED_3D_SECURE','FRAUD_RULE',\n",
        "            'SECURITY_LEVEL','EXTERNAL_RETURN_CODE','TRANSMITTER_COUNTRY',\n",
        "            'BUYER_IP_COUNTRY_CODE','IS_DUPLICATED','SHORT_MESSAGE',\n",
        "            'FRAUD_LIST','FRAUD_EXPLANATION','CHARGE_BACK','IS_CVD']\n",
        "threshold = 100\n",
        "similarity = 3\n",
        "df_groupedBy = do_clustring_presonnalized_algo(df_trans, columns, threshold, similarity)\n",
        "write_df_into_csv_file(gDrivePath+'/trans_clustred.csv', df_groupedBy)\n",
        "df_groupedByCluster = df_groupedBy.groupby(by=['Cluster', 'Counter'], as_index=False)['Counter'].count()\n",
        "write_df_into_csv_file(gDrivePath+'/nb_clusters.csv', df_groupedByCluster)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRKoOG_EFOIH"
      },
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "plt.bar(np.arange(df_groupedBy.shape[0]), df_groupedBy['Counter'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xjuSDxZRRcs"
      },
      "source": [
        "### extract the apropriate columns\n",
        "file_path = gDrivePath+'/fdj_transactions.csv'\n",
        "df_trans = read_csv_file_into_df(file_path)\n",
        "columns=['IS_DUPLICATED', 'ACTION','PAYMENT_CARD_CODE', 'SHORT_MESSAGE',\n",
        "          'EXTERNAL_RETURN_CODE', 'FRAUD_LIST', 'FRAUD_RULE',\n",
        "          'FRAUD_EXPLANATION', 'FRAUD_ACTION', 'ENROLLED_3D_SECURE',\n",
        "          'AUTHENTICATED_3D_SECURE', 'SECURITY_LEVEL', 'CHARGE_BACK',\n",
        "          'IS_CVD', 'TRANSMITTER_COUNTRY', 'BUYER_IP_COUNTRY_CODE',\n",
        "          'OPERATING_SYSTEM', 'BROWSER']\n",
        "\n",
        "# columns=['IS_DUPLICATED', 'ACTION', 'PAYMENT_CARD_CODE' 'SHORT_MESSAGE',\n",
        "#           'EXTERNAL_RETURN_CODE', 'FRAUD_LIST',\n",
        "#           'FRAUD_EXPLANATION', 'FRAUD_ACTION', 'ENROLLED_3D_SECURE',\n",
        "#           'AUTHENTICATED_3D_SECURE', 'SECURITY_LEVEL', 'CHARGE_BACK',\n",
        "#           'IS_CVD']\n",
        "\n",
        "df_trans = filter_df(df_trans, columns)\n",
        "df_trans.fillna('vide', inplace=True)\n",
        "write_df_into_csv_file(gDrivePath+'/fdj_trans_filterd_columns.csv', df_trans)\n",
        "\n",
        "\n",
        "# df_trans = preprocessing_convert_df_to_numerical_values(df_trans)\n",
        "# cluster_labels = do_kmeans(df_trans, 5)\n",
        "# df_trans = do_PCA(df_trans, 2)\n",
        "# plot_clusters_2D(df_trans, cluster_labels, (20, 15))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPWfojYqjJnY"
      },
      "source": [
        "### Apply Density based clustring : DBSCAN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a9IcX14IERT"
      },
      "source": [
        "## statistics\n",
        "df_trans = read_csv_file_into_df(gDrivePath+'/fdj_transactions.csv')\n",
        "\n",
        "df_trans['Counter'] = 1\n",
        "\n",
        "df_stat_OPERATING_SYSTEM = df_trans.groupby(by=['EXTERNAL_RETURN_CODE'], as_index=False)['Counter'].count()\n",
        "\n",
        "print(df_stat_OPERATING_SYSTEM)\n",
        "plt.figure(figsize=(20,30))\n",
        "plt.scatter(df_stat_OPERATING_SYSTEM.index, df_stat_OPERATING_SYSTEM['Counter'], c=df_stat_OPERATING_SYSTEM.index)\n",
        "plt.legend()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R45LSUyHDlfh"
      },
      "source": [
        "write_df_into_csv_file(gDrivePath+'/stat_EXTERNAL_RETURN_CODE.csv', df_stat_OPERATING_SYSTEM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV4FZNFoQoZ1"
      },
      "source": [
        "df_stat_OPERATING_SYSTEM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LGJp-1BPQUS"
      },
      "source": [
        "### KMode for clustring categorical data : clustring transactions\n",
        "### create the dataset that contains the centroids and there clusters\n",
        "# do training\n",
        "kmodes_estimator = KModes(n_clusters=4, init='Huang')\n",
        "cluster_labels = kmodes_estimator.fit_predict(X)\n",
        "\n",
        "# get centroids\n",
        "centroids = kmodes_estimator.cluster_centroids_\n",
        "\n",
        "# get centroids clusters\n",
        "centroids_cluster = kmodes_estimator.predict(centroids)\n",
        "\n",
        "# concatenate centroids with there adequate clusters\n",
        "centroid_dataset = np.concatenate((centroids, centroids_cluster.reshape(centroids_cluster.shape[0], 1)), axis=1)\n",
        "\n",
        "\n",
        "# create the dataframe that contains the centroids for each cluster with \n",
        "# apropriate cluster\n",
        "columns = list(X.columns)\n",
        "columns.append('Cluster')\n",
        "df_centroid = pd.DataFrame(data=centroid_dataset, columns=columns)\n",
        "write_df_into_csv_file('trans_centroids.csv', df_centroid )\n",
        "\n",
        "\n",
        "fdj_transactions_with_clusters = np.concatenate((X, cluster_labels.reshape(cluster_labels.shape[0], 1)), axis=1)\n",
        "columns = list(X.columns).append('Cluster')\n",
        "fdj_transactions_with_clusters = pd.DataFrame(fdj_transactions_with_clusters,columns=columns)\n",
        "write_df_into_csv_file('fdj_transaction_with_cluster.csv', fdj_transactions_with_clusters)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEo1RN_OnE5h"
      },
      "source": [
        "### I can't apply KNN because i have categorical data and that's gonna take\n",
        "### a lot of time from me, instead I am gonna define a similarity function\n",
        "### to get the distance between two rows\n",
        "\n",
        "\n",
        "def calculate_similarity(a, b):\n",
        "  return a.eq(b).sum()\n",
        "\n",
        "\n",
        "def get_coverage(df_centroids, df_trans_test):\n",
        "  coverage = df_centroids.shape[0]*[0]\n",
        "  non_matches = 0 \n",
        "  for i_index, i_row in df_trans_test.iterrows():\n",
        "    for j_index, j_row in df_centroid.iterrows():\n",
        "      similarity = calculate_similarity(i_row, j_row)\n",
        "      if similarity == 0 :\n",
        "        non_matches += 1\n",
        "      else:\n",
        "        coverage[j_index] += similarity\n",
        "  return coverage, non_matches\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_trans_test = read_csv_file_into_df('fdj_trans_test.csv')\n",
        "df_centroids = read_csv_file_into_df('trans_centroids.csv')\n",
        "df_centroids = df_centroid.filter(items=df_trans_test.columns)\n",
        "\n",
        "\n",
        "\n",
        "coverage, non_matches = get_coverage(df_centroids, df_trans_test)\n",
        "\n",
        "print(f'Coverage : {coverage}, \\n Non matches {non_matches}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr-7EhgwhMUE"
      },
      "source": [
        "### Silhouette metric for KModes (personal implementation)\n",
        "# you should call the function like that (see below)\n",
        "# calculate_dissimilarity(X[0:1].reset_index(drop=True), X[2:3].reset_index(drop=True)) \n",
        "def calculate_dissimilarity(a, b):\n",
        "  \"\"\"\n",
        "  Calculate the dissimlarity between two rows of a dataframe\n",
        "  a : row 1 (vecteur)\n",
        "  b : row 2 (vecteur)\n",
        "  return : value of dissimlarity\n",
        "  \"\"\"\n",
        "  return np.sum(a!=b)\n",
        "\n",
        "def kmodes_silhouette_sample_score(X, cluster_labels):\n",
        "  clusterd_dataset = np.concatenate((X, cluster_labels.reshape(cluster_labels.shape[0], 1)), axis=1)\n",
        "  \n",
        "  print('shape clustred dataset : ', clusterd_dataset.shape)\n",
        "  print(clusterd_dataset[:,-1:])\n",
        "  for i in set(cluster_labels):\n",
        "    samples_ith_cluster = np.where(clusterd_dataset[:,-1:] == i)\n",
        "    print(samples_ith_cluster)\n",
        "  # scores_list = np.array([])\n",
        "  # for i_index, i_row in X.iterrows():\n",
        "  #   a = 0 # distance inter_cluster\n",
        "  #   nb_inter_cluster = 1\n",
        "  #   b = 0 # distance intra_cluster\n",
        "  #   for j_index, j_row in X.iterrows():\n",
        "  #     if i_index != j_index:\n",
        "  #       if cluster_labels[i_index] == cluster_labels[j_index]:\n",
        "  #         a += calculate_dissimilarity(i_row, j_row)\n",
        "  #         nb_inter_cluster += 1\n",
        "  #       else:\n",
        "  #         b += calculate_dissimilarity(i_row, j_row)\n",
        "    \n",
        "  #   a /= nb_inter_cluster\n",
        "  #   b /= (X.shape[0] - nb_inter_cluster)\n",
        "  #   scores_list = np.append(scores_list,[(b - a) / max(b, a)])\n",
        "  # return scores_list\n",
        "\n",
        "def kmodes_silhouette_samples_mean(X, cluster_labels):\n",
        "  samples_avg_score = []\n",
        "  scores_list = kmodes_silhouette_sample_score(X, cluster_labels)\n",
        "  # find all sample'score for the ith cluster\n",
        "  for i in list(set(cluster_labels)):\n",
        "    same_cluster_samples_scores = scores_list[i == cluster_labels]\n",
        "    samples_avg_score.append(same_cluster_samples_scores.sum() / same_cluster_samples_scores.size)\n",
        "  return samples_avg_score\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clqJshIIycbY"
      },
      "source": [
        "## don't run it\n",
        "\n",
        "# silhouette technique useful just for KMeans not KModes because we need to have numeric values\n",
        "# Find the perfect K for the KMeans : Example for test\n",
        "# to generate a samples for testing the Kmeans\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "# to use the silhouette coefficient (the more the coefficient is heigh \n",
        "# more the clusters are good)\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# for chosing the color\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "kmeans_estimator = KMeans(n_clusters=5)\n",
        "\n",
        "\n",
        "n_clusters_range = [2, 3]\n",
        "\n",
        "y_start = 10\n",
        "for n_cluster in n_clusters_range:\n",
        "    fig, (ax) = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    ax.set_xlim([-0.1, 1])\n",
        "    \n",
        "\n",
        "    kmode_estimator = KModes(n_clusters=n_cluster, random_state=10)\n",
        "    # get the labels (the cluster for each sample)\n",
        "    cluster_labels = kmode_estimator.fit_predict(X)\n",
        "    # test something\n",
        "    centroids = kmode_estimator.cluster_centroids_\n",
        "    perdiction_centroids = kmode_estimator.predict(centroids)\n",
        "    # calculate the silhouette score for each sample\n",
        "    samples_silhouette_score = kmodes_silhouette_sample_score(X, cluster_labels)\n",
        "    print('type samples output: ', type(samples_silhouette_score))\n",
        "\n",
        "    # calculate mean score for each class \n",
        "    silhouette_avg = kmodes_silhouette_samples_mean(X, cluster_labels)\n",
        "\n",
        "    for i in range(n_cluster):\n",
        "      # get the silhouette score values for the cluster number i\n",
        "      ith_cluster_samples_silhouette_score = samples_silhouette_score[cluster_labels == i]\n",
        "      ith_cluster_samples_silhouette_score.sort()\n",
        "\n",
        "      y_end = y_start + ith_cluster_samples_silhouette_score.size\n",
        "      color = cm.nipy_spectral(float(i) / n_cluster)\n",
        "      ax.fill_betweenx(np.arange(y_start, y_end),\n",
        "                          0, ith_cluster_samples_silhouette_score,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "      \n",
        "      # Label the silhouette plots with their cluster numbers at the middle\n",
        "      size_cluster_i = ith_cluster_samples_silhouette_score.size\n",
        "      ax.text(-0.05, y_start + 0.5 * size_cluster_i, str(i))\n",
        "      ax.set_title(\"The silhouette plot for the various clusters.\")\n",
        "      ax.set_xlabel(\"The silhouette coefficient values\")\n",
        "      ax.set_ylabel(\"Cluster label\")\n",
        "\n",
        "      # The vertical line for average silhouette score of all the values\n",
        "      ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "      ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "      ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "      y_start = y_end + 10\n",
        "\n",
        "      plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_cluster),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd__y1qtlhX2"
      },
      "source": [
        "## scripts for test\n",
        "\n",
        "X = pd.DataFrame([['a', 'b', 'c', 'd'],\n",
        "                  ['a', 'b', 'c', 'g'],\n",
        "                  ['a', 'b', 'd', 'k'],\n",
        "                  ['a', 'b', 'c', 'd'],\n",
        "                  ['a', 'b', 'c', 'g'],\n",
        "                  ['a', 'b', 'd', 'k']], columns=['A', 'B', 'C', 'D'])\n",
        "\n",
        "kmodes = KModes(n_clusters=2, random_state=10)\n",
        "cluster_labels = kmodes.fit_predict(X)\n",
        "\n",
        "new_one = np.concatenate((X, cluster_labels.reshape(cluster_labels.shape[0], 1)), axis=1)\n",
        "another = np.where(new_one[:,-2:-1] == 'd')\n",
        "for i, j in zip(another[0], another[1]):\n",
        "  print(new_one[i:i+1])\n",
        "# print(kmodes_silhouette_sample_score(X, cluster_labels))\n",
        "\n",
        "# print(kmodes_silhouette_samples_mean(X, cluster_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C20VLqmMvN5t"
      },
      "source": [
        "# Exemple type\n",
        "# silhouette technique useful just for KMeans not KModes because we need to have numeric values\n",
        "# Find the perfect K for the KMeans : Example for test\n",
        "# to generate a samples for testing the Kmeans\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "# to use the silhouette coefficient (the more the coefficient is heigh \n",
        "# more the clusters are good)\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# for chosing the color\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# for kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "df_trans = read_csv_file_into_df('fdj_clustred_data.csv')\n",
        "df_trans.drop(columns=['Cluster'], inplace=True)\n",
        "\n",
        "n_clusters_range = [5, 10, 15, 20, 25, 30, 40]\n",
        "\n",
        "y_start = 10\n",
        "for n_cluster in n_clusters_range:\n",
        "    fig, (ax) = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(10, 7)\n",
        "\n",
        "    ax.set_xlim([-0.1, 1])\n",
        "    \n",
        "\n",
        "    kmean_estimator = KMeans(n_clusters=n_cluster, random_state=10)\n",
        "    # get the labels (the cluster for each sample)\n",
        "    cluster_labels = kmean_estimator.fit_predict(df_trans)\n",
        "    # calculate the silhouette score for each sample\n",
        "    samples_silhouette_score = silhouette_samples(df_trans, cluster_labels)\n",
        "    print('type samples output: ', type(samples_silhouette_score))\n",
        "\n",
        "    # calculate mean score for each class \n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "\n",
        "    for i in range(n_cluster):\n",
        "      # get the silhouette score values for the cluster number i\n",
        "      ith_cluster_samples_silhouette_score = samples_silhouette_score[cluster_labels == i]\n",
        "      ith_cluster_samples_silhouette_score.sort()\n",
        "\n",
        "      y_end = y_start + ith_cluster_samples_silhouette_score.size\n",
        "      color = cm.nipy_spectral(float(i) / n_cluster)\n",
        "      ax.fill_betweenx(np.arange(y_start, y_end),\n",
        "                          0, ith_cluster_samples_silhouette_score,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "      \n",
        "      # Label the silhouette plots with their cluster numbers at the middle\n",
        "      size_cluster_i = ith_cluster_samples_silhouette_score.size\n",
        "      ax.text(-0.05, y_start + 0.5 * size_cluster_i, str(i))\n",
        "      ax.set_title(\"The silhouette plot for the various clusters.\")\n",
        "      ax.set_xlabel(\"The silhouette coefficient values\")\n",
        "      ax.set_ylabel(\"Cluster label\")\n",
        "\n",
        "      # The vertical line for average silhouette score of all the values\n",
        "      ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "      ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "      ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "      y_start = y_end + 10\n",
        "\n",
        "      plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_cluster),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOK1Aw-jSVQO"
      },
      "source": [
        "# Exemple type\n",
        "# silhouette technique useful just for KMeans not KModes because we need to have numeric values\n",
        "# Find the perfect K for the KMeans : Example for test\n",
        "# to generate a samples for testing the Kmeans\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "# to use the silhouette coefficient (the more the coefficient is heigh \n",
        "# more the clusters are good)\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# for chosing the color\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# for kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X, y = make_blobs(n_samples=200000,\n",
        "                  n_features=2,\n",
        "                  centers=4,\n",
        "                  cluster_std=1,\n",
        "                  center_box=(-10.0, 10.0),\n",
        "                  shuffle=True,\n",
        "                  random_state=1)  # For reproducibility\n",
        "\n",
        "n_clusters_range = [2, 3, 4, 5, 6]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_start = 10\n",
        "for n_cluster in n_clusters_range:\n",
        "    fig, (ax) = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(10, 7)\n",
        "\n",
        "    ax.set_xlim([-0.1, 1])\n",
        "    \n",
        "\n",
        "    kmean_estimator = KMeans(n_clusters=n_cluster, random_state=10)\n",
        "    # get the labels (the cluster for each sample)\n",
        "    cluster_labels = kmean_estimator.fit_predict(X)\n",
        "    # calculate the silhouette score for each sample\n",
        "    samples_silhouette_score = silhouette_samples(X, cluster_labels)\n",
        "    print('type samples output: ', type(samples_silhouette_score))\n",
        "\n",
        "    # calculate mean score for each class \n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "\n",
        "    for i in range(n_cluster):\n",
        "      # get the silhouette score values for the cluster number i\n",
        "      ith_cluster_samples_silhouette_score = samples_silhouette_score[cluster_labels == i]\n",
        "      ith_cluster_samples_silhouette_score.sort()\n",
        "\n",
        "      y_end = y_start + ith_cluster_samples_silhouette_score.size\n",
        "      color = cm.nipy_spectral(float(i) / n_cluster)\n",
        "      ax.fill_betweenx(np.arange(y_start, y_end),\n",
        "                          0, ith_cluster_samples_silhouette_score,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "      \n",
        "      # Label the silhouette plots with their cluster numbers at the middle\n",
        "      size_cluster_i = ith_cluster_samples_silhouette_score.size\n",
        "      ax.text(-0.05, y_start + 0.5 * size_cluster_i, str(i))\n",
        "      ax.set_title(\"The silhouette plot for the various clusters.\")\n",
        "      ax.set_xlabel(\"The silhouette coefficient values\")\n",
        "      ax.set_ylabel(\"Cluster label\")\n",
        "\n",
        "      # The vertical line for average silhouette score of all the values\n",
        "      ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "      ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "      ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "      y_start = y_end + 10\n",
        "\n",
        "      plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_cluster),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSojW96ziDLF"
      },
      "source": [
        "### Ploting the clusters \n",
        "\n",
        "#Importing required modules\n",
        " \n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        " \n",
        "#Load Data\n",
        "data = load_digits().data\n",
        "pca = PCA(2)\n",
        " \n",
        "\n",
        "#Transform the data\n",
        "df = pca.fit_transform(data)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQR2QsyO9NHh"
      },
      "source": [
        "#Import required module\n",
        "from sklearn.cluster import KMeans\n",
        " \n",
        "#Initialize the class object\n",
        "kmeans = KMeans(n_clusters= 10)\n",
        " \n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df)\n",
        " \n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx9rCu9J9ZMF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n",
        "#Getting unique labels\n",
        " \n",
        "u_labels = np.unique(label)\n",
        " \n",
        "#plotting the results:\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "for i in u_labels:\n",
        "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnu2jMgS2nlu"
      },
      "source": [
        "### Mapping the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylBm981F9qRX"
      },
      "source": [
        "### Mapping categorial variables to numeric ones\n",
        "\n",
        "# Attention : don't forget to replace all the values in external_return_code by a value different from the one you assigned to 0\n",
        "# Attention : supprimer le IS_CVD\n",
        "# Attention : Pour transmitter_country, remplacer tout sauf FRA par 2 \n",
        "# Attention : Supprimer l'Operating et le browser\n",
        "\n",
        "mapping_columns = {\n",
        "    'ACTION': {\n",
        "      'AUTHOR+CAP':  1,\n",
        "      'AUTHOR':  2,\n",
        "      'RESET':  3,\n",
        "      'REFUND':  4\n",
        "    },\n",
        "    'PAYMENT_CARD_CODE':{\n",
        "      'CB': 1,\n",
        "      'PAYPAL': 2,\n",
        "      'PAYSAFECARD2':3,\n",
        "      'SKRILL(MONEYBOOKERS)': 4\n",
        "    },\n",
        "    'SHORT_MESSAGE':{\n",
        "      'REFUSED': 4,\n",
        "      'ACCEPTED': 1,\n",
        "      'ERROR': 4,\n",
        "      'CANCELLED': 3,\n",
        "      'ONHOLD_PARTNER': 2\n",
        "    },\n",
        "    'FRAUD_LIST':{\n",
        "      'StandardList': 1,\n",
        "      'NewCustomerList': 1,\n",
        "      'BlackList': 2\n",
        "    },\n",
        "    'FRAUD_RULE':{\n",
        "      'Cartes Business': 1,\n",
        "      'Cartes Corporate': 1,\n",
        "      'Montant 3DS-v2-FLS': 2,\n",
        "      'Cumul7j 3DS-v2-NCH': 3,\n",
        "      'OneCardNCustomers': 4,\n",
        "      'ListCountryIssuer': 5,\n",
        "      'Unicité du compte Paypal par compte joueur': 6,\n",
        "      'Paysafecard - Cumul client > 1000 EUR par jour calendaire': 7,\n",
        "      'Unicité du compte Skrill par compte joueur': 8,\n",
        "      'Paysafecard - Cumul client > 10000 EUR par mois': 9\n",
        "    },\n",
        "    'FRAUD_EXPLANATION': {\n",
        "      'AFF': 1,\n",
        "      '3DS': 1,\n",
        "      'CUC': 1,\n",
        "      'UNI': 1,\n",
        "      'CTY': 1,\n",
        "      'PSF': 1\n",
        "    },\n",
        "    'FRAUD_ACTION': {\n",
        "      'BLOCK': 5,\n",
        "      'FRICTIONLESS': 3,\n",
        "      'NONE': 1,\n",
        "      'NO_PREFERENCE': 4,\n",
        "      'noAction': 2\n",
        "    },\n",
        "    'ENROLLED_3D_SECURE':{\n",
        "      'Y': 1,\n",
        "      'N': 3\n",
        "    },\n",
        "    'AUTHENTICATED_3D_SECURE': {\n",
        "      'Y': 1,\n",
        "      'R': 2,\n",
        "      'N': 6,\n",
        "      'A': 3,\n",
        "      'U': 5\n",
        "    },\n",
        "    'SECURITY_LEVEL': {\n",
        "      'CVV': 1,\n",
        "      '3DS': 2,\n",
        "      'Aucun': 3\n",
        "    },\n",
        "    'CHARGE_BACK': {\n",
        "        'N': 1\n",
        "    },\n",
        "    'TRANSMITTER_COUNTRY':{\n",
        "        'FRA': 1\n",
        "    },\n",
        "    'BUYER_IP_COUNTRY_CODE':{\n",
        "        'FR': 1\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}